Matrix factorization is a type of collaborative filtering algorithm that can be used to build a recommendation system for users based on user feedback of objects (in this case songs). This allows us to recommend songs to users based on their listening history without the need for content based approaches. 
Training and testing matrices were constructed, of which both have $N_1$ users (rows denoted by $u_i$) and $N_2$ songs (columns denoted by $v_j$). As users have only listened to a fraction of all the songs available, the matrices are sparse, containing zeros in all entries except for those in which a user (a user representing a single row) has listened to. The goal of matrix factorization is to factor the training matrix into the product of two matrices $U$ and $V$.  The matrix $U$ is $N_1 \times d$ and the matrix $V$ is $d \times N_2$. Another goal of matrix factorization is typically to learn a low-rank factorization of the original matrix while restricting the patterns that are generated in the product of the factorized matrix (e.g. if a user likes a song in a specific group of songs, the user may also like other songs in that group of songs). The choice is $d$ can be subjective but $20$ is a common value to start with. In the factorized matrices, the predicted rating will be $\hat{M}_{ij} = u_i^\TRANSPOSE  v_j$
Using a coordinate ascent algorithm over 100 iterations, each row ($u_i$) and column ($v_j$) of the training matrix is then updated (equations \ref{eq1} and \ref{eq2}) in order to maximize the log joint likelihood (equation \ref{eq3}).
\begin{equation}
u_i = \left( \lambda\sigma^2 I + \sum_{j \in \Omega_{u_i}} v_j v_j^\TRANSPOSE \right)^{-1}\left(\sum_{j \in \Omega_{u_i}} M_{ij} v_{j} \right)
\label{eq1}
\end{equation}
\begin{equation}
v_j = \left( \lambda\sigma^2 I + \sum_{i \in \Omega_{v_j}} u_i u_i^\TRANSPOSE  \right)^{-1}\left(\sum_{j \in \Omega_{v_j}} M_{ij} u_{i} \right)
\label{eq2}
\end{equation}
\begin{equation}
\mathcal{L} = \sum_{(i,j) \in \Omega} \frac{1}{2\sigma^2} {|| M_{ij} - u_i^\TRANSPOSE  v_j||}^2 - \sum_{i=1}^{N_1} \frac{\lambda}{2} ||u_i^2 || - \sum_{i=1}^{N_2} \frac{\lambda}{2} ||v_j^2 || + \text{constant}
\label{eq3} 
\end{equation}

\emph{Note}: $\lambda = 10$ was used and $\Omega$ is the set of all indices in the matrix which have an observation.
We keep track of the root mean square error (RMSE) versus the testing set (i.e. how close our prediction is as compared to the actual normalized play count in the testing set) and the log joint likelihood of the training set as a function of iteration (see Figures \ref{fig:rmseplot} and \ref{fig:likeplot}).
There are several hyper parameters that can be tweaked in order to achieve a better mAP result using matrix factorization. The main ones that  was varied were the variance, the rank d of the factorization, and the iterations. As the project was being done using Python and the main computational package (scikit-learn) did not have probabilistic matrix factorization functions, we decided to apply the algorithm that we had wrote ourselves for a different class. 
 
After testing around with a few parameters, it was soon discovered that matrix factorization results were significantly below the popularity baseline and changing the hyperparameters around did not help much. The best MAP value of 0.0032 were achieved was using a high rank d of 80 over 100 iterations with a variance of 0.0001. The log joint likelihood chart can be used to verify that the probabilistic matrix factorization algorithm is running properly in that the values are monotonically decreasing which corresponds to a decreasing training error. The plot shows that there is more room for improving the training error, but the MAP plot that was taken from the same run shows that there is marginal difference in terms of improving the MAP score after the initial 5 iterations. Due to this result, further exploration of the probabilistic matrix factorization algorithm was limited to 30 iterations to reduce computational time. 
 
One explanation for the very low MAP scores using PMF was that the number of plays that were used are different from what PMF is typically used for which is bounded ratings. Using implicit user feedback (number of plays) instead of explicit user feedback (ratings) have been found in literature to create various issues. One of the main issue is that implicit feedback is inherently noisy does not measure negative feedback. Users may have played a song to simply find out that the dislike the song. The other main issue is that the exact number of plays has could have significantly different meaning measured between different users. For example, an avid user who has played a song five times could have different meaning to a user that has at most only played songs five times.
 
In order to tackle the issue of using implicit user feedback, we tried four different scaling schemas independently. These include normalizing user play counts by the total number of plays for the user, normalizing song play counts by the total number of plays for the song, making the data binary (changing all non-zero play counts to 1), and also applying tfidf. While testing out these schemas, we continued to vary the hyperparameters d and variance in order to determine if they changed the MAP results significantly. 
 
From the experiments, it was found that the rank d did not contribute significantly to the MAP values especially in settings where the MAP values were close to the baseline. Higher variance values were found to actually significantly increase the MAP values. In terms of scaling schemas, making the data binary produced the best result, with the second best being normalizing the data against the total number of plays for each user. Perhaps it can be said that using number of plays introduces more noise than simply considering if the user has played the song or not. Interestingly enough, normalizing the data against total number of plays for each song performed the worst and this most likely relates to the fact that number of plays is more meaningful relative to each user rather than to each song.
 
Regardless, the best result achieved for PMF in the end was 0.0143 which was only slightly above the popularity baseline. Based on literature review, we have found that this is better or in line with what other people have achieved with using PMF on play counts. Nevertheless, this indicates that PMF is not suitable for the MSD dataset and this can be attributed to the implicit feedback issue or simply that the dataset is just too sparse. However, it is still possible to apply matrix factorization techniques to this dataset through using metadata and this has been demonstrated to be viable by (Dawen, Paisley et. Al.) which achieved MAP values exceeding 0.1.  


