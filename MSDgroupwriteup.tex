% \documentclass[12pt]{emulateapj}
 \documentclass[12pt,preprint]{aastex}
\usepackage[margin= 1.0in]{geometry}    % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper} % or letter or a5paper or ... etc
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage[]{epsfig,graphicx}
\usepackage{color}
\usepackage{verbatim}
\DeclareGraphicsRule{.tif}{png}{.jpg}{`convert Num1 `dirname Num1`/`basename Num1 .tif`.jpg}
\newcommand{\units}[1]{\ensuremath{\, \mathrm{#1}}}
\usepackage{enumitem}
\usepackage{natbib,twoopt}
\newcommand{\degree}{\ensuremath{^\circ}}
\citestyle{apj}
\bibliographystyle{aa}
\usepackage[maxfloats=25]{morefloats}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage{ amssymb }

\begin{document}
\title{Clustering of Last.fm Music Artists}

 \author{Jordan Rosenblum, Justin Law \& Erin Grand}
 \affil{Data Science Institute, Columbia University, New York, NY 10027}
 
\date{\today}             

\begin{abstract}
This is the abstract. 
\end{abstract}

\tableofcontents

\section{Introduction}

\section{Data Munging}

\subsection{Matrix Factorization}
The goal of matrix factorization is to use collaborative methods to build a recommendation system for users based on user ratings of objects (in this case songs). This will allow us to recommend certain songs to users based on their listening history and without the need for using content based approaches. Since our data set contains number of plays for a given user and song (rather than rating), we normalized plays for every song on a scale between 0 and 1 and used this as a proxy for rating. 

% Erin's part here on normalization.

We then constructed training and testing matrices, of which both have $N_1$ users (rows denoted by $u_i$) and $N_2$ songs (columns denoted by $v_j$). Of course the matrices will be very sparse, containing zeros in all entries except for those in which a user (rows of matrix) has listened to a song (columns of matrix). The goal is to factor the training matrix into the product of two matrixes, $U$ and $V$. The matrix $U$ will be $N_1 \times d$ and the matrix $V$ will be $d \times N_2$. We want to learn a low-rank factorization (i.e. we choose $d$) so as to restrict the patterns we see in the rows and columns of our original matrix (e.g. we think a priori that if a user likes 1 top 100 song, the user may also like other top 100 songs). There is subjectivity in picking $d$ but $20$ is a common place to start. In the factorized matrices, the predicted rating will be $\hat{M}_{ij} = u_i^T v_j$ 

Using a coordinate ascent algorithm over 100 iterations, each row ($u_i$) and column ($v_j$) of the training matrix is then updated (see equations \ref{eq1} and \ref{eq2}) in order to maximize the log joint likelihood (see equation \ref{eq3}). 

\begin{equation}
u_i = \left( \lambda\sigma^2 I + \sum_{j \in \Omega_{w_i}} v_j v_j^T \right)^{-1}\left(\sum_{j \in \Omega_{w_i}} M_{ij} v_{j} \right)
\label{eq1}
\end{equation}


\begin{equation}
v_j = \left( \lambda\sigma^2 I + \sum_{i \in \Omega_{v_j}} u_i u_i^T \right)^{-1}\left(\sum_{j \in \Omega_{v_j}} M_{ij} u_{i} \right)
\label{eq2}
\end{equation}


\begin{equation}
\mathcal{L} = \sum_{(i,j) \in \Omega} \frac{1}{2\sigma^2} {|| M_{ij} - u_i^T v_j||}^2 - \sum_{i=1}^{N_1} \frac{\lambda}{2} ||u_i^2 || - \sum_{i=1}^{N_2} \frac{\lambda}{2} ||v_j^2 || + \text{constant}
\label{eq3} 
\end{equation}

Note: We use a rank 20 matrix for factorization purposes, a $\lambda = 10$, and calculate the variance of the observations for our $\sigma^2$. Also, $\Omega$ is the set of all indices in the matrix which have an observation.

We keep track of the root mean square error (RMSE) versus the testing set (i.e. how close our prediction is as compared to the actual normalized play count in the testing set) and the log joint likelihood of the training set as a function of iteration (see Figures \ref{fig:rmseplot} and \ref{fig:likeplot}).


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{h845BuzFO9CNgAAAABJRU5ErkJggg==.png} 
   \caption{(how close our prediction is as compared to the actual normalized play count in the testing set }
   \label{fig:rmseplot}
\end{figure}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=2in]{C9LAAAAABJRU5ErkJggg==.png} 
   \caption{log joint likelihood of the training set as a function of iteration }
   \label{fig:likeplot}
\end{figure}


\subsection{Kmeans Clustering}
Next, we cluster each of the learned $u_i$ using k-means (describe more here later) so that the cluster centroids can be interpreted as listener/personality types.

\section{Notes}

\href{http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/}{Music Recommendation Dataset}

Sparsity of $8.08199453451\times 10^{-5}$ (needs updating)

The matrix was too big for our computers to handle the matrix factorization using the whole set of users and songs, so we subset the data. 

The full matrix was originally $163206$ songs x $110000$ users. 

Subset songs by how many times the song was listened to. i.e the popularity of the song
= new number 

Subset users by how many songs they've listened to (how many popular song?)
= new number 

SONG PLAY COUNT  \\
mean = 28 \\
max = 35432 \\
min = 1 \\
std = 215.826789\\

USER PLAY COUNT\\
mean = 42\\
max = 1305\\
min = 5\\
std = 53.31547\\

29 - users
34 - songs 

How to split into training and test? 
Random selection by user or element? ELEMENT. 
Need matrixes to be the same size to compare them.

Interesting plots:
- Cumulative Distribution of number of songs with a given play count


\section{Appendix}
\href{https://github.com/eringrand/musicanalysis}{Link to our github repository} 
\href{http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/}{Link to the data set}


\end{document} 